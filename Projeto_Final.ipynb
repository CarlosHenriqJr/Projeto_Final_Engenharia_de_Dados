{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".appName('Projeto Final(Engenharia de Dados) - Painel Covid-19')\\\n",
    ".config('spark.some.config.option', 'some-value')\\\n",
    ".enableHiveSupport()\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "painelcovid = spark.read.csv('hdfs://namenode/user/Carlos/Projeto/Projeto', \n",
    "                                   sep=\";\", #sep(padrão ,): define o único caractere como separador para cada campo e valor.\n",
    "                                   header=True, #header(padrão false): usa a primeira linha como nomes de colunas.\n",
    "                                   inferSchema=True, #inferSchema(padrão false): infere o esquema de entrada automaticamente a partir dos dados. \n",
    "                                   ignoreLeadingWhiteSpace=True, #ignoreLeadingWhiteSpace(padrão false): define se os espaços em branco iniciais dos valores que estão sendo lidos devem ser ignorados.\n",
    "                                   ignoreTrailingWhiteSpace=True) #ignoreTrailingWhiteSpace(padrão false): define se os espaços em branco à direita dos valores que estão sendo lidos devem ser ignorados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- coduf: integer (nullable = true)\n",
      " |-- codmun: integer (nullable = true)\n",
      " |-- codRegiaoSaude: integer (nullable = true)\n",
      " |-- nomeRegiaoSaude: string (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      " |-- semanaEpi: integer (nullable = true)\n",
      " |-- populacaoTCU2019: integer (nullable = true)\n",
      " |-- casosAcumulado: decimal(10,0) (nullable = true)\n",
      " |-- casosNovos: integer (nullable = true)\n",
      " |-- obitosAcumulado: integer (nullable = true)\n",
      " |-- obitosNovos: integer (nullable = true)\n",
      " |-- Recuperadosnovos: integer (nullable = true)\n",
      " |-- emAcompanhamentoNovos: integer (nullable = true)\n",
      " |-- interior/metropolitana: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_painel_covidbr.dtypes\n",
    "painelcovid.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Otimizar dados por tabelas particionadas por município\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS basecovid\")\n",
    "painelcovid.write\\\n",
    ".mode('overwrite')\\\n",
    ".partitionBy('municipio')\\\n",
    ".format('hive')\\\n",
    ".saveAsTable('basecovid.municipio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5299 items\r\n",
      "-rw-r--r--   2 root supergroup          0 2022-08-08 05:17 hdfs://namenode:8020/user/hive/warehouse/basecovid/_SUCCESS\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-08 05:13 hdfs://namenode:8020/user/hive/warehouse/basecovid/municipio=Abadia de Goiás\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-08 05:16 hdfs://namenode:8020/user/hive/warehouse/basecovid/municipio=Abadia dos Dourados\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-08 05:13 hdfs://namenode:8020/user/hive/warehouse/basecovid/municipio=Abadiânia\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-08 05:16 hdfs://namenode:8020/user/hive/warehouse/basecovid/municipio=Abaetetuba\r\n",
     .
     .
     .
      "drwxr-xr-x   - root supergroup          0 2022-08-08 05:16 hdfs://namenode:8020/user/hive/warehouse/basecovid/municipio=Áurea\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-08 05:16 hdfs://namenode:8020/user/hive/warehouse/basecovid/municipio=Ângulo\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-08 05:16 hdfs://namenode:8020/user/hive/warehouse/basecovid/municipio=Érico Cardoso\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-08 05:16 hdfs://namenode:8020/user/hive/warehouse/basecovid/municipio=Óbidos\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-08 05:16 hdfs://namenode:8020/user/hive/warehouse/basecovid/municipio=Óleo\r\n"
     ]
    }
   ],
   "source": [
    "#Confirmar se os dados foram salvos no diretório\n",
    "!hdfs dfs -ls 'hdfs://namenode:8020/user/hive/warehouse/basecovid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|   basecovid|\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Acessar os dados da tabela municipio do Banco de dados basecovid pelo SparkSQL\n",
    "spark.sql(\"SHOW DATABASES\").show() #Mostrar os anco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "| database|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|basecovid|municipio|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecionar o banco de dados basecovid \n",
    "spark.sql(\"USE basecovid\")\n",
    "\n",
    "# Acessar os dados da tabela municipio do Banco de dados basecovid pelo SparkSQL\n",
    "spark.sql(\"SHOW TABLES\").show() #Mostrar os anco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Recuperados|\n",
      "+-----------+\n",
      "|   32511634|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 1.1  Recuperados\n",
    "recuperados = spark.sql(\"SELECT Recuperadosnovos as Recuperados FROM municipio order by 1 desc limit 1\")\n",
    "recuperados.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|Em_Acompanhamento|\n",
      "+-----------------+\n",
      "|          3182910|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 1.2 Em Acompanhamento\n",
    "em_acompanhamento = spark.sql(\"SELECT emAcompanhamentoNovos as Em_Acompanhamento FROM municipio order by 1 desc limit 1\")\n",
    "em_acompanhamento.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|Casos_Acumulados|\n",
      "+----------------+\n",
      "|        33890428|\n",
      "+----------------+\n",
      "\n",
      "+-----------+\n",
      "|Casos_Novos|\n",
      "+-----------+\n",
      "|     298408|\n",
      "+-----------+\n",
      "\n",
      "+----------+\n",
      "|Incidencia|\n",
      "+----------+\n",
      "|    9999.6|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2.1 Acumulado\n",
    "casos_acumulados = spark.sql(\"SELECT casosAcumulado as Casos_Acumulados FROM municipio order by 1 desc limit 1\")\n",
    "casos_acumulados.show()\n",
    "\n",
    "#2.2 Casos novos\n",
    "novos_casos = spark.sql(\"SELECT casosNovos as Casos_Novos FROM municipio order by 1 desc limit 1\")\n",
    "novos_casos.show()\n",
    "\n",
    "#2.3 Incidência (Casos acumulados)\n",
    "incidencia = spark.sql(\"SELECT cast(((casosAcumulado*100000)/populacaoTCU2019) as decimal(5,1)) as Incidencia FROM municipio order by 1 desc limit 1\")\n",
    "incidencia.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|Obitos_Acumulados|\n",
      "+-----------------+\n",
      "|           679010|\n",
      "+-----------------+\n",
      "\n",
      "+------------+\n",
      "|Obitos_Novos|\n",
      "+------------+\n",
      "|        1308|\n",
      "+------------+\n",
      "\n",
      "+----------+\n",
      "|Letalidade|\n",
      "+----------+\n",
      "|     114.3|\n",
      "+----------+\n",
      "\n",
      "+-----------+\n",
      "|Mortalidade|\n",
      "+-----------+\n",
      "|      992.9|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3.1 Óbitos acumulados\n",
    "obitos_acumulados = spark.sql(\"SELECT obitosAcumulado as Obitos_Acumulados FROM municipio order by 1 desc limit 1\")\n",
    "obitos_acumulados.show()\n",
    "\n",
    "#3.2 Obitos novos\n",
    "obitos_novos = spark.sql(\"SELECT obitosNovos as Obitos_Novos FROM municipio order by 1 desc limit 1\")\n",
    "obitos_novos.show()\n",
    "\n",
    "#3.3 Letalidade\n",
    "letalidade = spark.sql(\"SELECT cast(((obitosAcumulado * 100) / casosAcumulado) as decimal(5,1)) as Letalidade FROM municipio order by 1 desc limit 1\")\n",
    "letalidade.show()\n",
    "\n",
    "#3.4 Mortalidade\n",
    "mortalidade = spark.sql(\"SELECT cast(((obitosAcumulado * 100000) / populacaoTCU2019) as decimal(4,1)) as Mortalidade FROM municipio order by 1 desc limit 1\")\n",
    "mortalidade.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "recuperados.write.format('csv').saveAsTable('Recuperados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_acompanhamento.write.format('csv').saveAsTable('Acompanhamento')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+\n",
      "| database|     tableName|isTemporary|\n",
      "+---------+--------------+-----------+\n",
      "|basecovid|acompanhamento|      false|\n",
      "|basecovid|     municipio|      false|\n",
      "|basecovid|   recuperados|      false|\n",
      "+---------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Visualizar as tabelas salvas \n",
    "spark.sql('SHOW TABLES').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvar a segunda visualização com formato parquet e compressão snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_acumulados.write.option('compression', 'snappy').parquet('hdfs://namenode/user/Carlos/Projeto/visualizacao/Acumulados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "novos_casos.write.option('compression', 'snappy').parquet('hdfs://namenode/user/Carlos/Projeto/visualizacao/novos_casos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "incidencia.write.option('compression', 'snappy').parquet('hdfs://namenode/user/Carlos/Projeto/visualizacao/incidencia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-08 05:34 hdfs://namenode/user/Carlos/Projeto/visualizacao/Acumulados\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-08 05:36 hdfs://namenode/user/Carlos/Projeto/visualizacao/incidencia\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-08-08 05:35 hdfs://namenode/user/Carlos/Projeto/visualizacao/novos_casos\r\n"
     ]
    }
   ],
   "source": [
    "#Visualizar os dodos na pasta do HDFS\n",
    "!hdfs dfs -ls 'hdfs://namenode/user/Carlos/Projeto/visualizacao'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|value                       |\n",
      "+----------------------------+\n",
      "|{\"obitos_Acumulados\":679010}|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3.1 Óbitos acumulados\n",
    "obitos_acumulados_json = obitos_acumulados.select(to_json(struct('obitos_Acumulados')).alias('value'))\n",
    "obitos_acumulados_json.show(n=1, truncate=False, vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|value                |\n",
      "+---------------------+\n",
      "|{\"obitos_Novos\":1308}|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3.2 Obitos novos\n",
    "obitos_novos_json = obitos_novos.select(to_json(struct('obitos_Novos')).alias('value'))\n",
    "obitos_novos_json.show(n=1, truncate=False, vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|value               |\n",
      "+--------------------+\n",
      "|{\"Letalidade\":114.3}|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3.3 Letalidade\n",
    "letalidade_json = letalidade.select(to_json(struct('Letalidade')).alias('value'))\n",
    "letalidade_json.show(n=1, truncate=False, vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|value                |\n",
      "+---------------------+\n",
      "|{\"Mortalidade\":992.9}|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3.4 Mortalidade\n",
    "mortalidade_json = mortalidade.select(to_json(struct('Mortalidade')).alias('value'))\n",
    "mortalidade_json.show(n=1, truncate=False, vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "obitos_acumulados.selectExpr('to_json(struct(*)) As value')\\\n",
    ".write.format('kafka')\\\n",
    ".option('kafka.bootstrap.servers', 'kafka:9092')\\\n",
    ".option('topic', 'topic-obtidos_acumulados')\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "obitos_novos.selectExpr('to_json(struct(*)) As value')\\\n",
    ".write\\\n",
    ".format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\",\"kafka:9092\")\\\n",
    ".option(\"topic\",\"topic-obitos_novos\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "letalidade.selectExpr('to_json(struct(*)) As value')\\\n",
    ".write\\\n",
    ".format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\",\"kafka:9092\")\\\n",
    ".option(\"topic\",\"topic-letalidade\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mortalidade.selectExpr('to_json(struct(*)) As value')\\\n",
    ".write\\\n",
    ".format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\",\"kafka:9092\")\\\n",
    ".option(\"topic\",\"topic-mortalidade\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------+-------------------+\n",
      "|      Regiao|   Casos|Obitos|        Atualizacao|\n",
      "+------------+--------+------+-------------------+\n",
      "|      Brasil|33890428|679010|2022-08-02 00:00:00|\n",
      "|Centro-Oeste| 1631836| 27200|2022-08-02 00:00:00|\n",
      "|    Nordeste| 1662253| 30377|2022-08-02 00:00:00|\n",
      "|       Norte|  812659| 18548|2022-08-02 00:00:00|\n",
      "|     Sudeste| 5927119|173036|2022-08-02 00:00:00|\n",
      "|         Sul| 2694860| 44525|2022-08-02 00:00:00|\n",
      "+------------+--------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = spark.sql('''SELECT regiao as Regiao, \n",
    "                     MAX(casosAcumulado) as Casos, \n",
    "                     MAX(obitosAcumulado) as Obitos, \n",
    "                     MAX(data) as Atualizacao \n",
    "                     FROM municipio \n",
    "                     GROUP BY regiao \n",
    "                     ORDER BY regiao''')\n",
    "\n",
    "spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_NODES='<ES_ENDPOINT>'\n",
    "ES_PORT='<ES_PORT>'\n",
    "ES_NET_HTTP_AUTH_USER='<USER>'\n",
    "ES_NET_HTTP_AUTH_PASS='<PASSWORD>'\n",
    "ES_NET_SLL='true'\n",
    "ES_NODE_WAN_ONLY='true'\n",
    "ES_WRITE_OPERATION = 'upsert'\n",
    "\n",
    "def get_elastic_config_options() -> dict:\n",
    "    return {\n",
    "        'es.nodes': ES_NODES,\n",
    "        'es.port': ES_PORT,\n",
    "        'es.net.http.auth.user': ES_NET_HTTP_AUTH_USER,\n",
    "        'es.net.http.auth.pass': ES_NET_HTTP_AUTH_PASS,\n",
    "        'es.net.sll': ES_NET_SLL,\n",
    "        'es.nodes.wan.only': ES_NODE_WAN_ONLY,\n",
    "        'es.write.operation': ES_WRITE_OPERATION\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-40-364139d3e15a>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-364139d3e15a>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    .mode('append')     .save('Recuperados')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "ELASTIC_OPTIONS = get_elastic_config_options()\n",
    "\n",
    "recuperados.write \\\n",
    "    .format('org.elasticsearch.spark.sql') \\\n",
    "    .options(**ELASTIC_OPTIONS)\n",
    "    .mode('append') \\\n",
    "    .save('Recuperados')\n",
    "\n",
    "casos_acumulados.write \\\n",
    "    .format('org.elasticsearch.spark.sql') \\\n",
    "    .options(**ELASTIC_OPTIONS)\n",
    "    .mode('append') \\\n",
    "    .save('Casos_Confirmados')\n",
    "\n",
    "obitos_acumulados.write \\\n",
    "    .format('org.elasticsearch.spark.sql') \\\n",
    "    .options(**ELASTIC_OPTIONS)\n",
    "    .mode('append') \\\n",
    "    .save('Obitos_Confirmados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

